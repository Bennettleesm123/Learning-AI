# Import the numpy library for handling numbers and arrays.
import numpy as np

# Define a function to train the perceptron.
# X: the input data (each row is a data point with features)
# y: the correct answers (labels: 1 or -1)
# learning_rate: how much the weights adjust on each mistake
# epochs: number of times the algorithm goes through the entire dataset
def perceptron_train(X, y, learning_rate=1.0, epochs=10):
    n_samples, n_features = X.shape
    # Initialize weights as zeros.
    # We add one extra weight for the bias (a constant value that helps the model learn)
    weights = np.zeros(n_features + 1)
    
    # Loop through the training process several times (epochs)
    for epoch in range(epochs):
        print(f"Epoch {epoch + 1}")
        errors = 0  # Count misclassifications in this epoch
        
        # Process each data point one by one
        for idx in range(n_samples):
            # For each sample, add a 1 at the start of the feature list for the bias term
            xi = np.insert(X[idx], 0, 1)  
            
            # Calculate the weighted sum (dot product of weights and input)
            linear_output = np.dot(weights, xi)
            
            # Determine the prediction:
            # If the sum is 0 or more, predict 1; if less than 0, predict -1.
            prediction = 1 if linear_output >= 0 else -1
            
            # Check if the prediction is incorrect
            if prediction != y[idx]:
                # Calculate the change (update) for the weights.
                # The update is proportional to the difference between the true label and our prediction.
                update = learning_rate * (y[idx] - prediction)
                # Adjust the weights using the update value.
                weights += update * xi
                errors += 1
                print(f"  Sample {idx} misclassified. Updated weights: {weights}")
        
        # If there are no errors, the perceptron has learned to classify all samples correctly.
        if errors == 0:
            print("No errors in this epoch. Training complete.")
            break
    return weights

# Define a small, simple dataset.
# Each data point has two features.
# The rule is: if the sum of the two features is more than 3, label it 1; otherwise, label it -1.
X = np.array([
    [2, 2],  # 2+2=4 -> label 1
    [3, 1],  # 3+1=4 -> label 1
    [1, 3],  # 1+3=4 -> label 1
    [1, 1],  # 1+1=2 -> label -1
    [0, 2],  # 0+2=2 -> label -1
    [2, 0]   # 2+0=2 -> label -1
])
y = np.array([1, 1, 1, -1, -1, -1])  # These are the correct labels for our data

print("Training the perceptron model...")
# Train the model using our dataset.
weights = perceptron_train(X, y, learning_rate=1, epochs=10)
print("\nFinal weights after training:", weights)

# Define a function to make predictions using the trained perceptron.
def perceptron_predict(X, weights):
    predictions = []
    # Loop through each sample in the dataset
    for x in X:
        # Add the bias term to the sample.
        xi = np.insert(x, 0, 1)
        # Calculate the weighted sum.
        linear_output = np.dot(weights, xi)
        # Return 1 if the sum is 0 or more, else return -1.
        prediction = 1 if linear_output >= 0 else -1
        predictions.append(prediction)
    return predictions

# Use the trained perceptron to predict the labels for our training data.
print("\nMaking predictions on the training data:")
predictions = perceptron_predict(X, weights)
for i, pred in enumerate(predictions):
    print(f"  Data point {X[i]} predicted as {pred} (true label: {y[i]})")
